{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1df57b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using numpy 1.23.5\n",
      "Using pandas 2.0.3\n",
      "Using scikit-learn 1.2.2\n",
      "==== Legal Case Priority Classifier ====\n",
      "Enter path to data file (CSV or Excel): C:\\Users\\dell\\Downloads\\Indian_Court_Cases_Dataset_Updated.xlsx\n",
      "\n",
      "Data loaded successfully with 50 rows and 15 columns.\n",
      "\n",
      "Training models...\n",
      "Training completed successfully.\n",
      "\n",
      "Model Evaluation Results:\n",
      "\n",
      "Logistic Regression Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      游댮 High       0.00      0.00      0.00         3\n",
      "    游리 Medium       0.40      0.67      0.50         3\n",
      "       游릭 Low       0.25      0.25      0.25         4\n",
      "\n",
      "    accuracy                           0.30        10\n",
      "   macro avg       0.22      0.31      0.25        10\n",
      "weighted avg       0.22      0.30      0.25        10\n",
      "\n",
      "Accuracy: 0.30\n",
      "\n",
      "Random Forest Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      游댮 High       0.00      0.00      0.00         3\n",
      "    游리 Medium       0.00      0.00      0.00         3\n",
      "       游릭 Low       0.33      0.75      0.46         4\n",
      "\n",
      "    accuracy                           0.30        10\n",
      "   macro avg       0.11      0.25      0.15        10\n",
      "weighted avg       0.13      0.30      0.18        10\n",
      "\n",
      "Accuracy: 0.30\n",
      "\n",
      "Select number of cases to display:\n",
      "1. Top 10 highest priority cases\n",
      "2. Top 50 highest priority cases\n",
      "3. Top 100 highest priority cases\n",
      "4. All cases (ranked by priority)\n",
      "Enter your choice (1-4): 1\n",
      "\n",
      "==================================================\n",
      "\n",
      "Top 10 Priority Cases:\n",
      "    Priority_Score  Predicted_Priority Case_Status Past_History                          Case_Summary   Urgency_Tag Estimated_Impact Media_Coverage      Court_Name Hearing_Date   Legal_Sections Parties_Involved Case_ID        Advocate_Names Filing_Date Case_Type\n",
      "36        0.891286                   2     Pending          Yes  Brief description of the legal case.       Regular              Low            Yes      High Court   2023-07-17  Family Act 1955     Company vs X  C10036   K. Sharma, R. Mehta  2023-03-22       PIL\n",
      "2         0.864443                   2    Resolved          Yes  Brief description of the legal case.       Regular           Medium            Yes   Supreme Court   2024-07-24         CRPC 125     Company vs Z  C10002    L. Verma, R. Mehta  2023-08-28     Civil\n",
      "33        0.859301                   2    Resolved          Yes  Brief description of the legal case.       Regular             High             No   Supreme Court   2024-09-20       Article 32     Company vs Z  C10033   L. Verma, K. Sharma  2023-09-27     Civil\n",
      "46        0.857743                   2    Resolved          Yes  Brief description of the legal case.       Regular              Low            Yes   Supreme Court   2024-03-04  Family Act 1955     Company vs Z  C10046    L. Verma, A. Singh  2023-10-18     Civil\n",
      "9         0.827224                   2      Closed          Yes  Brief description of the legal case.       Regular              Low             No      High Court   2023-05-24       Article 32     Company vs X  C10009    R. Mehta, R. Mehta  2023-04-14     Civil\n",
      "28        0.821968                   0    Resolved           No  Brief description of the legal case.     Emergency              Low            Yes  District Court   2023-12-18  Family Act 1955     Company vs Y  C10028   R. Mehta, K. Sharma  2023-01-11    Family\n",
      "23        0.821950                   2     Pending           No  Brief description of the legal case.       Regular              Low            Yes      High Court   2023-08-18       Article 32        Govt vs Y  C10023    A. Singh, A. Singh  2023-02-20     Civil\n",
      "0         0.812261                   1    Resolved           No  Brief description of the legal case.     Emergency           Medium             No   Supreme Court   2023-09-29       Article 32     Company vs X  C10000    A. Singh, R. Mehta  2023-08-03    Family\n",
      "6         0.811915                   2      Closed          Yes  Brief description of the legal case.  High-profile              Low             No  District Court   2023-11-22  Family Act 1955     Company vs X  C10006   K. Sharma, L. Verma  2023-01-08     Civil\n",
      "21        0.811040                   1     Pending          Yes  Brief description of the legal case.       Regular             High             No   Supreme Court   2023-10-27     IPC 420, 467        Govt vs Y  C10021  K. Sharma, K. Sharma  2023-04-21    Family\n",
      "\n",
      "Save results to CSV? (y/n): n\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ====================== VERSION COMPATIBILITY CHECK ======================\n",
    "try:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn import __version__ as sklearn_version\n",
    "    \n",
    "    # Check versions\n",
    "    numpy_version = np.__version__\n",
    "    pandas_version = pd.__version__\n",
    "    \n",
    "    print(f\"Using numpy {numpy_version}\")\n",
    "    print(f\"Using pandas {pandas_version}\")\n",
    "    print(f\"Using scikit-learn {sklearn_version}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"Critical error: {str(e)}\")\n",
    "    print(\"Please install required packages using:\")\n",
    "    print(\"pip install numpy pandas scikit-learn openpyxl\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ====================== MAIN IMPORTS ======================\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# ====================== FUNCTION DEFINITIONS ======================\n",
    "def load_data():\n",
    "    \"\"\"Load data from CSV or Excel file with proper error handling\"\"\"\n",
    "    try:\n",
    "        file_path = input(\"Enter path to data file (CSV or Excel): \").strip()\n",
    "        if file_path.lower().endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif file_path.lower().endswith(('.xls', '.xlsx')):\n",
    "            df = pd.read_excel(file_path, engine='openpyxl')\n",
    "        else:\n",
    "            print(\"Error: Unsupported file type. Please provide a CSV or Excel file.\")\n",
    "            sys.exit(1)\n",
    "            \n",
    "        print(f\"\\nData loaded successfully with {len(df)} rows and {len(df.columns)} columns.\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {str(e)}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Clean and prepare the data for modeling.\"\"\"\n",
    "    if 'Priority_Label' not in df.columns:\n",
    "        print(\"Error: Required 'Priority_Label' column not found.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Clean and validate priority labels\n",
    "    df['Priority_Label'] = df['Priority_Label'].astype(str).str.strip()\n",
    "    valid_labels = ['High', 'Medium', 'Low', '游댮 High', '游리 Medium', '游릭 Low']\n",
    "    df = df[df['Priority_Label'].isin(valid_labels)]\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"Error: No valid priority labels found after cleaning.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Define feature types\n",
    "    FEATURE_DEFINITIONS = {\n",
    "        'Court_Name': 'categorical',\n",
    "        'Case_Type': 'categorical', \n",
    "        'Urgency_Tag': 'categorical',\n",
    "        'Advocate_Names': 'categorical',\n",
    "        'Legal_Sections': 'categorical',\n",
    "        'Past_History': 'categorical',\n",
    "        'Estimated_Impact': 'categorical',\n",
    "        'Media_Coverage': 'categorical',\n",
    "        'Days_to_Resolution': 'numerical'\n",
    "    }\n",
    "    \n",
    "    # Select available features\n",
    "    available_features = set(df.columns) - {'Priority_Label'}\n",
    "    categorical_cols = [col for col in available_features \n",
    "                       if FEATURE_DEFINITIONS.get(col, 'categorical') == 'categorical']\n",
    "    numerical_cols = [col for col in available_features \n",
    "                     if FEATURE_DEFINITIONS.get(col, 'categorical') == 'numerical']\n",
    "    \n",
    "    return df, categorical_cols, numerical_cols\n",
    "\n",
    "def train_models(X_train, y_train, categorical_cols, numerical_cols):\n",
    "    \"\"\"Train and return both logistic regression and random forest models.\"\"\"\n",
    "    # Preprocessing pipeline\n",
    "    numeric_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', numeric_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "    \n",
    "    # Model pipelines\n",
    "    lr_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(\n",
    "            max_iter=1000, \n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            multi_class='multinomial'\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    rf_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            class_weight='balanced'\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    print(\"\\nTraining models...\")\n",
    "    lr_pipeline.fit(X_train, y_train)\n",
    "    rf_pipeline.fit(X_train, y_train)\n",
    "    print(\"Training completed successfully.\")\n",
    "    \n",
    "    return lr_pipeline, rf_pipeline\n",
    "\n",
    "def evaluate_models(models, X_test, y_test):\n",
    "    \"\"\"Evaluate model performance on test data.\"\"\"\n",
    "    print(\"\\nModel Evaluation Results:\")\n",
    "    for name, model in models.items():\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(f\"\\n{name} Performance:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "\n",
    "def predict_and_rank(models, X, original_df, n_cases=None):\n",
    "    \"\"\"Generate predictions and rank cases by priority.\"\"\"\n",
    "    # Get predictions from both models\n",
    "    lr_proba = models['Logistic Regression'].predict_proba(X)\n",
    "    rf_proba = models['Random Forest'].predict_proba(X)\n",
    "    \n",
    "    # Average probabilities\n",
    "    avg_proba = (lr_proba + rf_proba) / 2\n",
    "    priority_scores = avg_proba.max(axis=1)\n",
    "    predicted_labels = avg_proba.argmax(axis=1)\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results = original_df.copy()\n",
    "    results['Priority_Score'] = priority_scores\n",
    "    results['Predicted_Priority'] = predicted_labels\n",
    "    \n",
    "    # Sort by priority score\n",
    "    results = results.sort_values('Priority_Score', ascending=False)\n",
    "    \n",
    "    return results.head(n_cases) if n_cases else results\n",
    "\n",
    "def get_user_selection():\n",
    "    \"\"\"Prompt user for number of cases to display.\"\"\"\n",
    "    print(\"\\nSelect number of cases to display:\")\n",
    "    print(\"1. Top 10 highest priority cases\")\n",
    "    print(\"2. Top 50 highest priority cases\")\n",
    "    print(\"3. Top 100 highest priority cases\")\n",
    "    print(\"4. All cases (ranked by priority)\")\n",
    "    \n",
    "    while True:\n",
    "        choice = input(\"Enter your choice (1-4): \")\n",
    "        if choice in ['1', '2', '3', '4']:\n",
    "            return {\n",
    "                '1': 10,\n",
    "                '2': 50,\n",
    "                '3': 100,\n",
    "                '4': None\n",
    "            }[choice]\n",
    "        print(\"Invalid input. Please enter 1-4.\")\n",
    "\n",
    "# ====================== MAIN EXECUTION ======================\n",
    "def main():\n",
    "    # Load and prepare data\n",
    "    print(\"==== Legal Case Priority Classifier ====\")\n",
    "    df = load_data()\n",
    "    df, categorical_cols, numerical_cols = preprocess_data(df)\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = df.drop(columns=['Priority_Label'])\n",
    "    y = df['Priority_Label']\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Train models\n",
    "    lr_model, rf_model = train_models(\n",
    "        X_train, y_train, categorical_cols, numerical_cols)\n",
    "    \n",
    "    # Evaluate models\n",
    "    evaluate_models({\n",
    "        'Logistic Regression': lr_model,\n",
    "        'Random Forest': rf_model\n",
    "    }, X_test, y_test)\n",
    "    \n",
    "    # Generate predictions\n",
    "    n_cases = get_user_selection()\n",
    "    ranked_cases = predict_and_rank(\n",
    "        {'Logistic Regression': lr_model, 'Random Forest': rf_model},\n",
    "        X, df, n_cases)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"\\n{'All' if n_cases is None else f'Top {n_cases}'} Priority Cases:\")\n",
    "    display_cols = ['Priority_Score', 'Predicted_Priority'] + categorical_cols + numerical_cols\n",
    "    print(ranked_cases[display_cols].to_string())\n",
    "    \n",
    "    # Save results option\n",
    "    if input(\"\\nSave results to CSV? (y/n): \").lower() == 'y':\n",
    "        output_path = input(\"Enter output filename (e.g., results.csv): \").strip()\n",
    "        ranked_cases.to_csv(output_path, index=False)\n",
    "        print(f\"Results saved to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "997e1564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69045b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_size(file_path):\n",
    "\n",
    "    size = os.path.getsize(file_path)\n",
    "\n",
    "    return size\n",
    "\n",
    "def convert_bytes(size, unit=None):\n",
    "\n",
    "    if unit == \"KB\":\n",
    "\n",
    "        return print('File size: str(round(size / 1024, 3)) + Kilobytes')\n",
    "\n",
    "    elif unit == \"MB\":\n",
    "\n",
    "        return print('File size: str(round(size/ (1024*1024), 3)) + Megabytes')\n",
    "\n",
    "    else:\n",
    "\n",
    "        return print('File size: + str(size) + bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94fe036c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to synchronously open file (file signature not found)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[1;32m----> 3\u001b[0m model\u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdell\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDownloads\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mCaseNext1.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m TF_LITE_MODEL_FILE_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtflite_model2.tflite\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m tf_lite_converter \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlite\u001b[38;5;241m.\u001b[39m TFLiteConverter\u001b[38;5;241m.\u001b[39mfrom_keras_model(model)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\saving\\saving_api.py:238\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    231\u001b[0m         filepath,\n\u001b[0;32m    232\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    233\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    234\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    235\u001b[0m     )\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    239\u001b[0m     filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    240\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\h5py\\_hl\\files.py:564\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[0;32m    555\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    556\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    557\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[0;32m    558\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[0;32m    559\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[0;32m    560\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    561\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[0;32m    562\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[0;32m    563\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 564\u001b[0m     fid \u001b[38;5;241m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[38;5;241m=\u001b[39mswmr)\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\h5py\\_hl\\files.py:238\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[0;32m    237\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 238\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    240\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to synchronously open file (file signature not found)"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model= load_model(r\"C:\\Users\\dell\\Downloads\\CaseNext1.h5\")\n",
    "\n",
    "TF_LITE_MODEL_FILE_NAME = \"tflite_model2.tflite\"\n",
    "\n",
    "tf_lite_converter = tf.lite. TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "tflite_model = tf_lite_converter.convert()\n",
    "\n",
    "tflite_model_name = TF_LITE_MODEL_FILE_NAME\n",
    "\n",
    "open(tflite_model_name, \"wb\").write(tflite_model)\n",
    "\n",
    "convert_bytes(get_file_size(TF_LITE_MODEL_FILE_NAME), \"KB\")\n",
    "\n",
    "#Convert the model.\n",
    "\n",
    "converter = tf.lite. TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "#or using another method\n",
    "\n",
    "#Save the model.\n",
    "\n",
    "with open('tflite_model_another2.tflite', 'wb') as f:\n",
    "\n",
    " f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3b1971",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
